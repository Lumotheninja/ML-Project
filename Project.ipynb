{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path, labeled=True):\n",
    "    with open(path, encoding=\"utf8\") as fp:\n",
    "        data = []\n",
    "        sentence = []\n",
    "        for line in fp:\n",
    "            if line == \"\\n\":\n",
    "                # start again\n",
    "                data.append(sentence)\n",
    "                sentence = []\n",
    "            else:\n",
    "                if labeled:\n",
    "                    tokens = line.strip().split()\n",
    "                    sentence.append((' '.join(tokens[:-1]), tokens[-1]))\n",
    "                else:\n",
    "                    sentence.append((line.strip(), ))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset(dataset, path):\n",
    "    with open(path, 'w') as fp:\n",
    "        for sentence in dataset:\n",
    "            for row in sentence:\n",
    "                fp.write(' '.join(row) + \"\\n\")\n",
    "            fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1: \n",
    "Write a function that estimates the emission parameters from the training set using MLE (maximum\n",
    "likelihood estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_e_without_smoothing(dataset):\n",
    "    # ys = set([y for sentence in dataset for (x, y) in sentence])\n",
    "    words = set([x for sentence in dataset for (x, y) in sentence])\n",
    "    count_ys = defaultdict(int)\n",
    "    count_y_x = defaultdict(lambda  : defaultdict(int))\n",
    "    for sentence in dataset:\n",
    "        for word, label in sentence:\n",
    "            count_ys[label] += 1\n",
    "            count_y_x[label][word] += 1\n",
    "    #es = {label: {x: count/count_ys[label] for x, count in emissions.iteritems()} for label, emissions in count_y_x.iteritems()}\n",
    "    def e(x, y):\n",
    "        return count_y_x[y][x]/count_ys[y]\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2:\n",
    "Set k to 1, implement this fix into your function for computing the emission parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_e(dataset, k=1):\n",
    "    # ys = set([y for sentence in dataset for (x, y) in sentence])\n",
    "    words = set([x for sentence in dataset for (x, y) in sentence])\n",
    "    count_ys = defaultdict(int)\n",
    "    count_y_x = defaultdict(lambda  : defaultdict(int))\n",
    "    for sentence in dataset:\n",
    "        for word, label in sentence:\n",
    "            count_ys[label] += 1\n",
    "            count_y_x[label][word] += 1\n",
    "    #es = {label: {x: count/count_ys[label] for x, count in emissions.iteritems()} for label, emissions in count_y_x.iteritems()}\n",
    "    def e(x, y):\n",
    "        if x in words:\n",
    "            return count_y_x[y][x]/(count_ys[y] + k)\n",
    "        else:\n",
    "            return k/(count_ys[y] + k)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3: \n",
    "Compare your outputs and the gold-standard outputs in dev.out\n",
    "and report the precision, recall and F scores of such a baseline system for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_labeling(dataset, train_set):\n",
    "    ys = set([y for sentence in train_set for (x, y) in sentence])\n",
    "    e = get_e(train_set)\n",
    "    output = []\n",
    "    for sentence in dataset:\n",
    "        labeled_sentence = []\n",
    "        for word,  in sentence:\n",
    "            max_p = 0\n",
    "            label = None\n",
    "            for y in ys:\n",
    "                p = e(word, y)\n",
    "                if p > max_p:\n",
    "                    max_p = p\n",
    "                    label = y\n",
    "            labeled_sentence.append((word, label))\n",
    "        output.append(labeled_sentence)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = read_dataset('EN/train')\n",
    "en_dev = read_dataset('EN/dev.in', labeled=False)\n",
    "en_dev_labeled = simple_labeling(en_dev, en_train)\n",
    "write_dataset(en_dev_labeled, \"EN/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN\n",
    "## Entity in gold data: 802\n",
    "## Entity in prediction: 1148\n",
    "\n",
    "## Correct Entity : 614\n",
    "Entity  precision: 0.5348\n",
    "\n",
    "Entity  recall: 0.7656\n",
    "\n",
    "Entity  F: 0.6297\n",
    "\n",
    "## Correct Entity Type : 448\n",
    "Entity Type  precision: 0.3902\n",
    "\n",
    "Entity Type  recall: 0.5586\n",
    "\n",
    "Entity Type  F: 0.4595\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_train = read_dataset('FR/train')\n",
    "fr_dev = read_dataset('FR/dev.in', labeled=False)\n",
    "fr_dev_labeled = simple_labeling(fr_dev, fr_train)\n",
    "write_dataset(fr_dev_labeled, \"FR/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FR\n",
    "\n",
    "## Entity in gold data: 238\n",
    "## Entity in prediction: 1114\n",
    "\n",
    "## Correct Entity : 186\n",
    "Entity  precision: 0.1670\n",
    "\n",
    "Entity  recall: 0.7815\n",
    "\n",
    "Entity  F: 0.2751\n",
    "\n",
    "## Correct Entity Type : 79\n",
    "Entity Type  precision: 0.0709\n",
    "\n",
    "Entity Type  recall: 0.3319\n",
    "\n",
    "Entity Type  F: 0.1169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u300a' in position 0: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-71ad3b7f0def>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcn_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CN/dev.in'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcn_dev_labeled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_labeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcn_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwrite_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_dev_labeled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"CN/dev.p2.out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a2cf7802eaa1>\u001b[0m in \u001b[0;36mwrite_dataset\u001b[1;34m(dataset, path)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python 3.5\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u300a' in position 0: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "cn_train = read_dataset('CN/train')\n",
    "cn_dev = read_dataset('CN/dev.in', labeled=False)\n",
    "cn_dev_labeled = simple_labeling(cn_dev, cn_train)\n",
    "write_dataset(cn_dev_labeled, \"CN/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN\n",
    "## Entity in gold data: 1081\n",
    "## Entity in prediction: 5161\n",
    "\n",
    "## Correct Entity : 602\n",
    "Entity  precision: 0.1166\n",
    "\n",
    "Entity  recall: 0.5569\n",
    "\n",
    "Entity  F: 0.1929\n",
    "\n",
    "## Correct Entity Type : 354\n",
    "\n",
    "Entity Type  precision: 0.0686\n",
    "\n",
    "Entity Type  recall: 0.3275\n",
    "\n",
    "Entity Type  F: 0.1134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u266b' in position 0: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-563060c704d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msg_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SG/dev.in'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msg_dev_labeled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_labeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msg_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwrite_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msg_dev_labeled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SG/dev.p2.out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a2cf7802eaa1>\u001b[0m in \u001b[0;36mwrite_dataset\u001b[1;34m(dataset, path)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python 3.5\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u266b' in position 0: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "sg_train = read_dataset('SG/train')\n",
    "sg_dev = read_dataset('SG/dev.in', labeled=False)\n",
    "sg_dev_labeled = simple_labeling(sg_dev, sg_train)\n",
    "write_dataset(sg_dev_labeled, \"SG/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SG\n",
    "## Entity in gold data: 4092\n",
    "## Entity in prediction: 12125\n",
    "\n",
    "## Correct Entity : 2394\n",
    "Entity  precision: 0.1974\n",
    "\n",
    "Entity  recall: 0.5850\n",
    "\n",
    "Entity  F: 0.2952\n",
    "\n",
    "## Correct Entity Type : 1217\n",
    "Entity Type  precision: 0.1004\n",
    "\n",
    "Entity Type  recall: 0.2974\n",
    "\n",
    "Entity Type  F: 0.1501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1: \n",
    "Write a function that estimates the transition parameters from the training set using MLE (maximum\n",
    "likelihood estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q(dataset, k=1):\n",
    "    qs = defaultdict(lambda  : defaultdict(int))\n",
    "    count_ys = defaultdict(int)\n",
    "    count_ys['START'] = len(dataset)\n",
    "    count_ys['STOP'] = len(dataset)\n",
    "    for sentence in dataset:\n",
    "        n = len(sentence)\n",
    "        for i in range(n):\n",
    "            current_y = sentence[i][1]\n",
    "            count_ys[current_y] += 1\n",
    "            if i == 0:\n",
    "                qs[current_y]['START'] += 1\n",
    "            elif i == n-1:\n",
    "                qs['STOP'][current_y] += 1\n",
    "                qs[current_y][sentence[i-1][1]] += 1\n",
    "            else:\n",
    "                qs[current_y][sentence[i-1][1]] += 1\n",
    "    def q(yi, yi_1):\n",
    "        if qs[yi][yi_1] == 0:\n",
    "            return k/(count_ys[yi_1] + k) # smoothing\n",
    "        return qs[yi][yi_1]/(count_ys[yi_1] + k)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2:\n",
    "Use the estimated transition and emission parameters, implement the Viterbi algorithm taught in class\n",
    "to compute the following (for a sentence with n words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def hmm(sentence, q, e, ys):\n",
    "    pi = [{}]\n",
    "    for y in ys:\n",
    "        pi[-1][y] = np.log(0)\n",
    "    pi[-1]['START'] = 0\n",
    "    \n",
    "    n = len(sentence)\n",
    "    for i in range(n):\n",
    "        x = sentence[i][0]\n",
    "        pi.append({})\n",
    "        for y in ys:\n",
    "            pi[-1][y] = max([p + np.log(q(y, u)) + np.log(e(x, y)) for u, p in pi[-2].items()])\n",
    "    pi.append({})\n",
    "    pi[-1]['STOP'] = max([pi[-2][u] + np.log(q('STOP', u)) for u in ys])\n",
    "    # return pi\n",
    "    labels = [None] * n\n",
    "    next_ = 'STOP'\n",
    "    for i in range(n, 0, -1):\n",
    "        max_p = np.log(0)\n",
    "        best_y = None\n",
    "        for y in ys:\n",
    "            lgp = pi[i][y] + np.log(q(next_, y))\n",
    "            if lgp >= max_p:\n",
    "                max_p = lgp\n",
    "                best_y = y\n",
    "        next_ = best_y\n",
    "        labels[i - 1] = best_y\n",
    "    return list(zip([word for word,  in sentence], labels))\n",
    "\n",
    "def label_with_hmm(dataset, train_set, k=1, get_q=get_q, get_e=get_e):\n",
    "    ys = set([y for sentence in train_set for (x, y) in sentence])\n",
    "    q = get_q(train_set, k=k)\n",
    "    e = get_e(train_set, k=k)\n",
    "    return [hmm(sentence, q, e, ys) for sentence in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dev_labeled_hmm = label_with_hmm(en_dev, en_train)\n",
    "write_dataset(en_dev_labeled_hmm, \"EN/dev.p3.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN\n",
    "## Entity in gold data: 802\n",
    "## Entity in prediction: 956\n",
    "\n",
    "## Correct Entity : 564\n",
    "Entity  precision: 0.5900\n",
    "\n",
    "Entity  recall: 0.7032\n",
    "\n",
    "Entity  F: 0.6416\n",
    "\n",
    "## Correct Entity Type : 451\n",
    "Entity Type  precision: 0.4718\n",
    "\n",
    "Entity Type  recall: 0.5623\n",
    "\n",
    "Entity Type  F: 0.5131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dev_labeled_hmm = label_with_hmm(fr_dev, fr_train)\n",
    "write_dataset(fr_dev_labeled_hmm, \"FR/dev.p3.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FR\n",
    "## Entity in gold data: 238\n",
    "## Entity in prediction: 448\n",
    "\n",
    "## Correct Entity : 137\n",
    "Entity  precision: 0.3058\n",
    "\n",
    "Entity  recall: 0.5756\n",
    "\n",
    "Entity  F: 0.3994\n",
    "\n",
    "## Correct Entity Type : 76\n",
    "Entity Type  precision: 0.1696\n",
    "\n",
    "Entity Type  recall: 0.3193\n",
    "\n",
    "Entity Type  F: 0.2216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u300a' in position 0: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-19ac503028ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcn_dev_labeled_hmm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_with_hmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcn_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwrite_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_dev_labeled_hmm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"CN/dev.p3.out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a2cf7802eaa1>\u001b[0m in \u001b[0;36mwrite_dataset\u001b[1;34m(dataset, path)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python 3.5\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u300a' in position 0: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "cn_dev_labeled_hmm = label_with_hmm(cn_dev, cn_train)\n",
    "write_dataset(cn_dev_labeled_hmm, \"CN/dev.p3.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN\n",
    "## Entity in gold data: 1081\n",
    "## Entity in prediction: 1221\n",
    "\n",
    "## Correct Entity : 456\n",
    "Entity  precision: 0.3735\n",
    "\n",
    "Entity  recall: 0.4218\n",
    "\n",
    "Entity  F: 0.3962\n",
    "\n",
    "## Correct Entity Type : 309\n",
    "Entity Type  precision: 0.2531\n",
    "\n",
    "Entity Type  recall: 0.2858\n",
    "\n",
    "Entity Type  F: 0.2685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_dev_labeled_hmm = label_with_hmm(sg_dev, sg_train)\n",
    "write_dataset(sg_dev_labeled_hmm, \"SG/dev.p3.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SG\n",
    "## Entity in gold data: 4092\n",
    "## Entity in prediction: 4118\n",
    "\n",
    "## Correct Entity : 1720\n",
    "Entity  precision: 0.4177\n",
    "\n",
    "Entity  recall: 0.4203\n",
    "\n",
    "Entity  F: 0.4190\n",
    "\n",
    "## Correct Entity Type : 1039\n",
    "Entity Type  precision: 0.2523\n",
    "\n",
    "Entity Type  recall: 0.2539\n",
    "\n",
    "Entity Type  F: 0.2531"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1:\n",
    "Describe the Viterbi algorithm used for decoding such a second-order HMM model and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_order_q(dataset, k=1):\n",
    "    qs = defaultdict(lambda  : defaultdict(lambda: defaultdict(int))) # {yi: {yi_1: {yi_2: prob}}}\n",
    "    count_ys = defaultdict(lambda: defaultdict(int)) # {yi_1: {yi_2: count}}\n",
    "    count_ys['START']['START'] = len(dataset)\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        n = len(sentence)\n",
    "        for i in range(n):\n",
    "            current_y = sentence[i][1]\n",
    "            if i == 0:\n",
    "                count_ys[current_y]['START'] += 1\n",
    "            else:\n",
    "                prev_y = sentence[i-1][1]\n",
    "                count_ys[current_y][prev_y] += 1\n",
    "            \n",
    "            if i == 0:\n",
    "                qs[current_y]['START']['START'] += 1\n",
    "            elif i == 1:\n",
    "                qs[current_y][sentence[i-1][1]]['START'] += 1\n",
    "            elif i == n-1:\n",
    "                qs[current_y][sentence[i-1][1]][sentence[i-2][1]] += 1\n",
    "                qs['STOP'][current_y][sentence[i-1][1]] += 1\n",
    "            else:\n",
    "                qs[current_y][sentence[i-1][1]][sentence[i-2][1]] += 1\n",
    "                \n",
    "    def q(yi, yi_1, yi_2):\n",
    "        if qs[yi][yi_1][yi_2] == 0:\n",
    "            if count_ys[yi_1][yi_2] == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return k/(count_ys[yi_1][yi_2] + k) # smoothing\n",
    "        return qs[yi][yi_1][yi_2]/(count_ys[yi_1][yi_2] + k)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_hmm(sentence, q, e, ys):\n",
    "    pi = [defaultdict(lambda: defaultdict(int)), defaultdict(lambda: defaultdict(int))] # array of {v: {u: prob}}\n",
    "    pi[0]['START']['START'] = 1\n",
    "    \n",
    "    for y in ys:\n",
    "        pi[1][y]['START'] = q(y, 'START', 'START') * e(sentence[0][0], y)\n",
    "    \n",
    "    n = len(sentence)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        word = sentence[i][0]\n",
    "        \n",
    "        pi.append(defaultdict(lambda : defaultdict(int)))\n",
    "        \n",
    "        for v in ys:\n",
    "            for u in ys:\n",
    "                pi[-1][v][u] = max([p * q(v, u, uprime) * e(word, v) for uprime, p in pi[-2][u].items()])\n",
    "        \n",
    "        \n",
    "    pi.append(defaultdict(lambda: defaultdict(int)))\n",
    "    for y in ys:\n",
    "        pi[-1]['STOP'][y] = max([p * q('STOP', y, u) for u, p in pi[-2][y].items()])\n",
    "    \n",
    "    output = [None] * n\n",
    "    next_next_y = None\n",
    "    next_y = None\n",
    "    \n",
    "    max_p = 0\n",
    "    for v in ys:\n",
    "        for u in ys:\n",
    "            p = pi[n][v][u] * q('STOP', v, u)\n",
    "            if p >= max_p:\n",
    "                next_next_y = v\n",
    "                next_y = u\n",
    "                max_p = p\n",
    "    \n",
    "    output[n-1] = next_next_y\n",
    "    output[n-2] = next_y\n",
    "    \n",
    "    for k in range(n-1, 1, -1):\n",
    "        word = sentence[k][0]\n",
    "        max_p = 0\n",
    "        best_u = None\n",
    "        for u in ys:\n",
    "            p = pi[k][next_y][u] * q(next_next_y, next_y, u) * e(word,next_next_y)\n",
    "            if p >= max_p:\n",
    "                max_p = p\n",
    "                best_u = u\n",
    "        next_next_y, next_y = next_y, best_u\n",
    "        output[k-2] = best_u\n",
    "    return list(zip([word for word, in sentence],output))\n",
    "\n",
    "def label_with_second_order_hmm(dataset, train_set, k=1, get_second_order_q=get_second_order_q, get_e=get_e):\n",
    "    ys = set([y for sentence in train_set for (x, y) in sentence])\n",
    "    q = get_second_order_q(train_set, k=k)\n",
    "    e = get_e(train_set, k=k)\n",
    "    return [second_order_hmm(sentence, q, e, ys) for sentence in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dev_labeled_second_order_hmm = label_with_second_order_hmm(en_dev, en_train)\n",
    "write_dataset(en_dev_labeled_second_order_hmm, \"EN/dev.p4.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN\n",
    "## Entity in gold data: 802\n",
    "## Entity in prediction: 976\n",
    "\n",
    "## Correct Entity : 562\n",
    "Entity  precision: 0.5758\n",
    "\n",
    "Entity  recall: 0.7007\n",
    "\n",
    "Entity  F: 0.6322\n",
    "\n",
    "## Correct Entity Type : 411\n",
    "Entity Type  precision: 0.4211\n",
    "\n",
    "Entity Type  recall: 0.5125\n",
    "\n",
    "Entity Type  F: 0.4623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dev_labeled_second_order_hmm = label_with_second_order_hmm(fr_dev, fr_train)\n",
    "write_dataset(fr_dev_labeled_second_order_hmm, \"FR/dev.p4.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FR\n",
    "## Entity in gold data: 238\n",
    "## Entity in prediction: 297\n",
    "\n",
    "## Correct Entity : 133\n",
    "Entity  precision: 0.4478\n",
    "\n",
    "Entity  recall: 0.5588\n",
    "\n",
    "Entity  F: 0.4972\n",
    "\n",
    "## Correct Entity Type : 77\n",
    "Entity Type  precision: 0.2593\n",
    "\n",
    "Entity Type  recall: 0.3235\n",
    "\n",
    "Entity Type  F: 0.2879"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Design challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the design challenge, we created a total of 3 models, first order HMM with JM smoothing, second order HMM with JM smoothing and third order HMM with JM smoothing. We used JM smoothing as there might be a lot of “holes” in the higher order HMM models as the probabilities get smaller, hence it might be a good idea to interpolate lower order models with higher order ones. It also ensures that unknown words can be handled in a smooth manner. After testing our models on the EN and FR dataset, we chose the first order HMM with JM smoothing as it was having the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jm_e(dataset, k=0.99):\n",
    "    words = set([x for sentence in dataset for (x, y) in sentence])\n",
    "    count_ys = defaultdict(int)\n",
    "    count_y_x = defaultdict(lambda  : defaultdict(int))\n",
    "    count_word = defaultdict(int)\n",
    "    for sentence in dataset:\n",
    "        for word, label in sentence:\n",
    "            count_word[word] += 1\n",
    "            count_ys[label] += 1\n",
    "            count_y_x[label][word] += 1\n",
    "    N = sum([len(sentence) for sentence in dataset])\n",
    "    def e(x, y):\n",
    "        return k * count_y_x[y][x]/(count_ys[y]) + (1-k)*k * count_word[x]/N +(1-k)**2 * 1/(len(words) + 1) # + 1 for UNK\n",
    "    return e\n",
    "\n",
    "def get_jm_q(dataset, k=0.99):\n",
    "    qs = defaultdict(lambda  : defaultdict(int))\n",
    "    count_ys = defaultdict(int)\n",
    "    count_ys['START'] = len(dataset)\n",
    "    count_ys['STOP'] = len(dataset)\n",
    "    for sentence in dataset:\n",
    "        n = len(sentence)\n",
    "        for i in range(n):\n",
    "            current_y = sentence[i][1]\n",
    "            count_ys[current_y] += 1\n",
    "            if i == 0:\n",
    "                qs[current_y]['START'] += 1\n",
    "            elif i == n-1:\n",
    "                qs['STOP'][current_y] += 1\n",
    "                qs[current_y][sentence[i-1][1]] += 1\n",
    "            else:\n",
    "                qs[current_y][sentence[i-1][1]] += 1\n",
    "    N = sum([len(sentence) for sentence in dataset])\n",
    "    def q(yi, yi_1):\n",
    "        return k * qs[yi][yi_1]/(count_ys[yi_1]) + (1-k) * count_ys[yi]/N\n",
    "    return q\n",
    "\n",
    "def get_second_order_jm_q(dataset, k=0.99):\n",
    "    qs = defaultdict(lambda  : defaultdict(lambda: defaultdict(int))) # {yi: {yi_1: {yi_2: prob}}}\n",
    "    count_ys = defaultdict(lambda: defaultdict(int)) # {yi_1: {yi_2: count}}\n",
    "    count_ys['START']['START'] = len(dataset)\n",
    "    count_y = defaultdict(int)\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        n = len(sentence)\n",
    "        for i in range(n):\n",
    "            current_y = sentence[i][1]\n",
    "            count_y[current_y] += 1\n",
    "            if i == 0:\n",
    "                count_ys[current_y]['START'] += 1\n",
    "            else:\n",
    "                prev_y = sentence[i-1][1]\n",
    "                count_ys[current_y][prev_y] += 1\n",
    "            \n",
    "            if i == 0:\n",
    "                qs[current_y]['START']['START'] += 1\n",
    "            elif i == 1:\n",
    "                qs[current_y][sentence[i-1][1]]['START'] += 1\n",
    "            elif i == n-1:\n",
    "                qs[current_y][sentence[i-1][1]][sentence[i-2][1]] += 1\n",
    "                qs['STOP'][current_y][sentence[i-1][1]] += 1\n",
    "            else:\n",
    "                qs[current_y][sentence[i-1][1]][sentence[i-2][1]] += 1\n",
    "    # N = sum([len(sentence) for sentence in dataset])           \n",
    "    first_order_q = get_jm_q(dataset, k=k)\n",
    "    def q(yi, yi_1, yi_2):\n",
    "        # base_rate = count_y[yi]/N\n",
    "        base_rate = first_order_q(yi, yi_1)\n",
    "        if count_ys[yi_1][yi_2] == 0:\n",
    "            return (1-k) * base_rate\n",
    "        return k * qs[yi][yi_1][yi_2]/(count_ys[yi_1][yi_2]) + (1-k) * base_rate\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st order HMM with JM smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dev_labeled_hmm_jm = label_with_hmm(en_dev, en_train, get_e=get_jm_e, get_q=get_jm_q, k=0.5)\n",
    "write_dataset(en_dev_labeled_hmm_jm, \"EN/dev.p3.jm.0.5.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dev_labeled_hmm_jm = label_with_hmm(fr_dev, fr_train, get_e=get_jm_e, get_q=get_jm_q, k=0.99)\n",
    "write_dataset(fr_dev_labeled_hmm_jm, \"FR/dev.p3.jm.0.99.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd order HMM with JM smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dev_labeled_second_order_hmm_jm = label_with_second_order_hmm(en_dev, en_train, get_e=get_jm_e, get_second_order_q=get_second_order_jm_q, k=0.5)\n",
    "write_dataset(en_dev_labeled_second_order_hmm_jm, \"EN/dev.p4.jm.0.5.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dev_labeled_second_order_hmm_jm = label_with_second_order_hmm(fr_dev, fr_train, get_e=get_jm_e, get_second_order_q=get_second_order_jm_q, k=0.99)\n",
    "write_dataset(fr_dev_labeled_second_order_hmm_jm, \"FR/dev.p4.jm.0.99.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd order HMM with JM smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jm_e(dataset, k=0.99):\n",
    "    words = set([x for sentence in dataset for (x, y) in sentence])\n",
    "    count_ys = defaultdict(int)\n",
    "    count_y_x = defaultdict(lambda  : defaultdict(int))\n",
    "    count_word = defaultdict(int)\n",
    "    for sentence in dataset:\n",
    "        for word, label in sentence:\n",
    "            count_word[word] += 1\n",
    "            count_ys[label] += 1\n",
    "            count_y_x[label][word] += 1\n",
    "    N = sum([len(sentence) for sentence in dataset])\n",
    "    def e(x, y):\n",
    "        return k * count_y_x[y][x]/(count_ys[y]) + (1-k)*k * count_word[x]/N +(1-k)**2 * 1/(len(words) + 1) # + 1 for UNK\n",
    "    return e\n",
    "\n",
    "def get_jm_q(dataset, k=0.99):\n",
    "    qs = defaultdict(lambda  : defaultdict(int))\n",
    "    count_ys = defaultdict(int)\n",
    "    count_ys['START'] = len(dataset)\n",
    "    count_ys['STOP'] = len(dataset)\n",
    "    for sentence in dataset:\n",
    "        n = len(sentence)\n",
    "        for i in range(n):\n",
    "            current_y = sentence[i][1]\n",
    "            count_ys[current_y] += 1\n",
    "            if i == 0:\n",
    "                qs[current_y]['START'] += 1\n",
    "            elif i == n-1:\n",
    "                qs['STOP'][current_y] += 1\n",
    "                qs[current_y][sentence[i-1][1]] += 1\n",
    "            else:\n",
    "                qs[current_y][sentence[i-1][1]] += 1\n",
    "    N = sum([len(sentence) for sentence in dataset])\n",
    "    def q(yi, yi_1):\n",
    "        return k * qs[yi][yi_1]/(count_ys[yi_1]) + (1-k) * count_ys[yi]/N\n",
    "    return q\n",
    "\n",
    "def get_second_order_jm_q(dataset, k=0.99):\n",
    "    qs = defaultdict(lambda  : defaultdict(lambda: defaultdict(int))) # {yi: {yi_1: {yi_2: prob}}}\n",
    "    count_ys = defaultdict(lambda: defaultdict(int)) # {yi_1: {yi_2: count}}\n",
    "    count_ys['START']['START'] = len(dataset)\n",
    "    count_y = defaultdict(int)\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        n = len(sentence)\n",
    "        for i in range(n):\n",
    "            current_y = sentence[i][1]\n",
    "            count_y[current_y] += 1\n",
    "            if i == 0:\n",
    "                count_ys[current_y]['START'] += 1\n",
    "            else:\n",
    "                prev_y = sentence[i-1][1]\n",
    "                count_ys[current_y][prev_y] += 1\n",
    "            \n",
    "            if i == 0:\n",
    "                qs[current_y]['START']['START'] += 1\n",
    "            elif i == 1:\n",
    "                qs[current_y][sentence[i-1][1]]['START'] += 1\n",
    "            elif i == n-1:\n",
    "                qs[current_y][sentence[i-1][1]][sentence[i-2][1]] += 1\n",
    "                qs['STOP'][current_y][sentence[i-1][1]] += 1\n",
    "            else:\n",
    "                qs[current_y][sentence[i-1][1]][sentence[i-2][1]] += 1\n",
    "    # N = sum([len(sentence) for sentence in dataset])           \n",
    "    first_order_q = get_jm_q(dataset, k=k)\n",
    "    def q(yi, yi_1, yi_2):\n",
    "        # base_rate = count_y[yi]/N\n",
    "        base_rate = first_order_q(yi, yi_1)\n",
    "        if count_ys[yi_1][yi_2] == 0:\n",
    "            return (1-k) * base_rate\n",
    "        return k * qs[yi][yi_1][yi_2]/(count_ys[yi_1][yi_2]) + (1-k) * base_rate\n",
    "    return q\n",
    "\n",
    "def get_third_order_jm_q(dataset, k=0.99):\n",
    "    qs = defaultdict(lambda  : defaultdict(lambda  : defaultdict(lambda: defaultdict(int)))) # {yi: {yi_1: {yi_2: {yi_3: prob}}}}\n",
    "    count_ys = defaultdict(lambda :defaultdict(lambda: defaultdict(int))) # {yi_1: {yi_2: {yi_3:count}}}\n",
    "    count_ys['START']['START']['START'] = len(dataset)\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        n = len(sentence)\n",
    "        for i in range(n):\n",
    "            current_y = sentence[i][1]\n",
    "            if i == 0:\n",
    "                count_ys[current_y]['START']['START'] += 1\n",
    "            elif i == 1:\n",
    "                prev_y = sentence[i-1][1]\n",
    "                count_ys[current_y][prev_y]['START'] += 1\n",
    "            else:\n",
    "                prev_y = sentence[i-1][1]\n",
    "                prev_2_y = sentence[i-2][1]\n",
    "                count_ys[current_y][prev_y][prev_2_y] += 1\n",
    "            \n",
    "            if i == 0:\n",
    "                qs[current_y]['START']['START']['START'] += 1\n",
    "            elif i == 1:\n",
    "                qs[current_y][sentence[i-1][1]]['START']['START'] += 1\n",
    "            elif i == 2:\n",
    "                qs[current_y][sentence[i-1][1]][sentence[i-2][1]]['START'] += 1\n",
    "            elif i == n-1:\n",
    "                qs[current_y][sentence[i-1][1]][sentence[i-2][1]][sentence[i-3][1]] += 1\n",
    "                qs['STOP'][current_y][sentence[i-1][1]][sentence[i-2][1]] += 1\n",
    "            else:\n",
    "                qs[current_y][sentence[i-1][1]][sentence[i-2][1]][sentence[i-3][1]] += 1\n",
    "    second_order_q = get_second_order_jm_q(dataset, k=k)          \n",
    "    def q(yi, yi_1, yi_2, yi_3):\n",
    "        base_rate = second_order_q(yi, yi_1, yi_2)\n",
    "        if qs[yi][yi_1][yi_2][yi_3] == 0:\n",
    "            return (1-k) * base_rate\n",
    "        return k * qs[yi][yi_1][yi_2][yi_3]/(count_ys[yi_1][yi_2][yi_3]) + (1-k) * base_rate\n",
    "    return q\n",
    "\n",
    "\n",
    "def label_with_third_order_hmm(dataset, train_set, k=1, get_third_order_q=get_third_order_jm_q, get_e=get_e):\n",
    "    ys = set([y for sentence in train_set for (x, y) in sentence])\n",
    "    q = get_third_order_jm_q(train_set, k=k)\n",
    "    e = get_e(train_set, k=k)\n",
    "    return [third_order_hmm(sentence, q, e, ys) for sentence in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_order_hmm(sentence, q, e, ys):\n",
    "    pi = [defaultdict(lambda: defaultdict(lambda: defaultdict(int))), defaultdict(lambda: defaultdict(lambda: defaultdict(int))), defaultdict(lambda: defaultdict(lambda: defaultdict(int)))] # array of {w: {v: {u: prob}}}, w being most recent, yi\n",
    "    pi[0]['START']['START']['START'] = 1\n",
    "    \n",
    "    # 1st layer\n",
    "    for y in ys:\n",
    "        pi[1][y]['START']['START'] = q(y, 'START', 'START', 'START') * e(sentence[0][0], y)\n",
    "    \n",
    "    # 2nd layer\n",
    "    for w in ys:\n",
    "        for v in ys:\n",
    "            pi[2][w][v]['START'] = max([p * q(w, v, 'START', vprime) * e(sentence[1][0], w) for vprime, p in pi[1][v]['START'].items()])\n",
    "            \n",
    "    n = len(sentence)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        word = sentence[i][0]\n",
    "        \n",
    "        pi.append(defaultdict(lambda: defaultdict(lambda: defaultdict(int))))\n",
    "        \n",
    "        for w in ys:\n",
    "            for v in ys:\n",
    "                for u in ys:\n",
    "                    pi[-1][w][v][u] = max([p * q(w, v, u, uprime) * e(word, w) for uprime, p in pi[-2][v][u].items()])\n",
    "        \n",
    "        \n",
    "    pi.append(defaultdict(lambda: defaultdict(lambda: defaultdict(int))))\n",
    "    for w in ys:\n",
    "        for v in ys:\n",
    "            pi[-1]['STOP'][w][v] = max([p * q('STOP', w, v, u) for u, p in pi[-2][w][v].items()])\n",
    "    \n",
    "    output = [None] * n\n",
    "    next_next_next_y = None\n",
    "    next_next_y = None\n",
    "    next_y = None\n",
    "    \n",
    "    max_p = 0\n",
    "    for w in ys:\n",
    "        for v in ys:\n",
    "            for u in ys:\n",
    "                p = pi[n][w][v][u] * q('STOP', w, v, u)\n",
    "                if p >= max_p:\n",
    "                    next_next_next_y = w\n",
    "                    next_next_y = v\n",
    "                    next_y = u\n",
    "                    max_p = p\n",
    "    \n",
    "    output[n-1] = next_next_next_y\n",
    "    output[n-2] = next_next_y\n",
    "    output[n-3] = next_y\n",
    "    \n",
    "    for k in range(n-1, 1, -1):\n",
    "        word = sentence[k][0]\n",
    "        max_p = 0\n",
    "        best_u = None\n",
    "        for u in ys:\n",
    "            p = pi[k][next_next_y][next_y][u] * q(next_next_next_y, next_next_y, next_y, u) * e(word,next_next_next_y)\n",
    "            if p >= max_p:\n",
    "                max_p = p\n",
    "                best_u = u\n",
    "        next_next_next_y, next_next_y, next_y = next_next_y, next_y, best_u\n",
    "        output[k-3] = best_u\n",
    "    return list(zip([word for word, in sentence],output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dev_labeled_3rd_order_hmm_jm = label_with_third_order_hmm(en_dev, en_train, get_e=get_jm_e, get_third_order_q=get_third_order_jm_q, k=0.99)\n",
    "write_dataset(en_dev_labeled_3rd_order_hmm_jm, \"EN/dev.p5.jm.0.99.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dev_labeled_3rd_order_hmm_jm = label_with_third_order_hmm(fr_dev, fr_train, get_e=get_jm_e, get_third_order_q=get_third_order_jm_q, k=0.99)\n",
    "write_dataset(fr_dev_labeled_3rd_order_hmm_jm, \"FR/dev.p5.jm.0.99.out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
